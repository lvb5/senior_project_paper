@techreport{bakerWorkshopReportBasic2019,
  title = {Workshop {{Report}} on {{Basic Research Needs}} for {{Scientific Machine Learning}}: {{Core Technologies}} for {{Artificial Intelligence}}},
  shorttitle = {Workshop {{Report}} on {{Basic Research Needs}} for {{Scientific Machine Learning}}},
  author = {Baker, Nathan and Alexander, Frank and Bremer, Timo and Hagberg, Aric and Kevrekidis, Yannis and Najm, Habib and Parashar, Manish and Patra, Abani and Sethian, James and Wild, Stefan and Willcox, Karen and Lee, Steven},
  year = {2019},
  month = feb,
  number = {1478744},
  pages = {1478744},
  institution = {{US Department of Energy}},
  doi = {10.2172/1478744},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/9UDU6AG6/Baker et al. - 2019 - Workshop Report on Basic Research Needs for Scient.pdf}
}

@article{benhammoudaAnalyticalSolutionsSystems2014,
  title = {Analytical Solutions for Systems of Partial Differential\textendash Algebraic Equations},
  author = {Benhammouda, Brahim and {Vazquez-Leal}, Hector},
  year = {2014},
  month = mar,
  journal = {SpringerPlus},
  volume = {3},
  number = {1},
  pages = {137},
  issn = {2193-1801},
  doi = {10.1186/2193-1801-3-137},
  abstract = {This work presents the application of the power series method (PSM) to find solutions of partial differential-algebraic equations (PDAEs). Two systems of index-one and index-three are solved to show that PSM can provide analytical solutions of PDAEs in convergent series form. What is more, we present the post-treatment of the power series solutions with the Laplace-Pad\'e (LP) resummation method as a useful strategy to find exact solutions. The main advantage of the proposed methodology is that the procedure is based on a few straightforward steps and it does not generate secular terms or depends of a perturbation parameter.},
  keywords = {Laplace transform,Padé approximant; Analytical solutions,Partial differential-algebraic equations,Power series method},
  file = {/Users/miles_cb/Zotero/storage/4NQ2ZXTB/Benhammouda and Vazquez-Leal - 2014 - Analytical solutions for systems of partial differ.pdf;/Users/miles_cb/Zotero/storage/NHG92DY4/2193-1801-3-137.html}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception},
  file = {/Users/miles_cb/Zotero/storage/MA27L3IB/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{blechschmidtThreeWaysSolve2021,
  title = {Three Ways to Solve Partial Differential Equations with Neural Networks \textemdash{} {{A}} Review},
  author = {Blechschmidt, Jan and Ernst, Oliver G.},
  year = {2021},
  journal = {GAMM-Mitteilungen},
  volume = {44},
  number = {2},
  pages = {e202100006},
  issn = {1522-2608},
  doi = {10.1002/gamm.202100006},
  abstract = {Neural networks are increasingly used to construct numerical solution methods for partial differential equations. In this expository review, we introduce and contrast three important recent approaches attractive in their simplicity and their suitability for high-dimensional problems: physics-informed neural networks, methods based on the Feynman\textendash Kac formula and methods based on the solution of backward stochastic differential equations. The article is accompanied by a suite of expository software in the form of Jupyter notebooks in which each basic methodology is explained step by step, allowing for a quick assimilation and experimentation. An extensive bibliography summarizes the state of the art.},
  langid = {english},
  keywords = {backward differential equation,curse of dimensionality,Feynman–Kac,Hamilton–Jacobi–Bellman equations,neural networks,partial differential equation,PINN,stochastic process},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/gamm.202100006},
  file = {/Users/miles_cb/Zotero/storage/Y29XVS2M/Blechschmidt and Ernst - 2021 - Three ways to solve partial differential equations.pdf;/Users/miles_cb/Zotero/storage/T3LFU7QE/gamm.html}
}

@article{brownGeoniumTheoryPhysics1986,
  title = {Geonium Theory: {{Physics}} of a Single Electron or Ion in a {{Penning}} Trap},
  shorttitle = {Geonium Theory},
  author = {Brown, Lowell S. and Gabrielse, Gerald},
  year = {1986},
  month = jan,
  journal = {Reviews of Modern Physics},
  volume = {58},
  number = {1},
  pages = {233--311},
  issn = {0034-6861},
  doi = {10.1103/RevModPhys.58.233},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/QVUD9RI4/Brown and Gabrielse - 1986 - Geonium theory Physics of a single electron or io.pdf}
}

@article{buddBlowupSystemPartial1994,
  title = {Blow-up in a {{System}} of {{Partial Differential Equations}} with {{Conserved First Integral}}. {{Part II}}: {{Problems}} with {{Convection}}},
  shorttitle = {Blow-up in a {{System}} of {{Partial Differential Equations}} with {{Conserved First Integral}}. {{Part II}}},
  author = {Budd, C. J. and Dold, J. W. and Stuart, A. M.},
  year = {1994},
  month = jun,
  journal = {SIAM Journal on Applied Mathematics},
  volume = {54},
  number = {3},
  pages = {610--640},
  issn = {0036-1399, 1095-712X},
  doi = {10.1137/S0036139992232131},
  abstract = {A reaction-diffusion-convection equation with a nonlocal term is studied; the nonlocal operator acts to conserve the spatial integral of the unknown function as time evolves. The equations are parameterised by \#, and for \# the equation arises as a similarity solution of the Navier-Stokes equations and the nonlocal term plays the role of pressure. For tt 0, the equation is a nonlocal reaction-diffusion problem. The aim of the paper is to determine for which values of the parameter tt blow-up occurs and to study its form. In particular, interest is focused on the three cases \# 5, , tt {$>$} 5, and \# 1.},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/IT2K8LSM/Budd et al. - 1994 - Blow-up in a System of Partial Differential Equati.pdf}
}

@book{chauvinBackpropagationTheoryArchitectures1995,
  title = {Backpropagation: {{Theory}}, {{Architectures}}, and {{Applications}}},
  shorttitle = {Backpropagation},
  editor = {Chauvin, Yves and Rumelhart, David E.},
  year = {1995},
  month = feb,
  publisher = {{Psychology Press}},
  address = {{New York}},
  doi = {10.4324/9780203763247},
  abstract = {Composed of three sections, this book presents the most popular training algorithm for neural networks: backpropagation. The first section presents the theory and principles behind backpropagation as seen from different perspectives such as statistics, machine learning, and dynamical systems. The second presents a number of network architectures that may be designed to match the general concepts of Parallel Distributed Processing with backpropagation learning. Finally, the third section shows how these principles can be applied to a number of different fields related to the cognitive sciences, including control, speech recognition, robotics, image processing, and cognitive psychology. The volume is designed to provide both a solid theoretical foundation and a set of examples that show the versatility of the concepts. Useful to experts in the field, it should also be most helpful to students seeking to understand the basic principles of connectionist learning and to engineers wanting to add neural networks in general -- and backpropagation in particular -- to their set of problem-solving methods.},
  isbn = {978-0-203-76324-7}
}

@misc{chenNeuralOrdinaryDifferential2019,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  number = {arXiv:1806.07366},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1806.07366},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/miles_cb/Zotero/storage/2WUDNIH6/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf;/Users/miles_cb/Zotero/storage/9ETKW9MY/1806.html}
}

@misc{choromanskaLossSurfacesMultilayer2015,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  year = {2015},
  month = jan,
  number = {arXiv:1412.0233},
  eprint = {1412.0233},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.0233},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/miles_cb/Zotero/storage/DP6UHUSC/Choromanska et al. - 2015 - The Loss Surfaces of Multilayer Networks.pdf;/Users/miles_cb/Zotero/storage/8X3NBTLT/1412.html}
}

@article{dubeyActivationFunctionsDeep2022a,
  title = {Activation Functions in Deep Learning: {{A}} Comprehensive Survey and Benchmark},
  shorttitle = {Activation Functions in Deep Learning},
  author = {Dubey, Shiv Ram and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
  year = {2022},
  month = sep,
  journal = {Neurocomputing},
  volume = {503},
  pages = {92--108},
  issn = {09252312},
  doi = {10.1016/j.neucom.2022.06.111},
  abstract = {Neural networks have shown tremendous growth in recent years to solve numerous problems. Various types of neural networks have been introduced to deal with different types of problems. However, the main goal of any neural network is to transform the non-linearly separable input data into more linearly separable abstract features using a hierarchy of layers. These layers are combinations of linear and nonlinear functions. The most popular and common non-linearity layers are activation functions (AFs), such as Logistic Sigmoid, Tanh, ReLU, ELU, Swish and Mish. In this paper, a comprehensive overview and survey is presented for AFs in neural networks for deep learning. Different classes of AFs such as Logistic Sigmoid and Tanh based, ReLU based, ELU based, and Learning based are covered. Several characteristics of AFs such as output range, monotonicity, and smoothness are also pointed out. A performance comparison is also performed among 18 state-of-the-art AFs with different networks on different types of data. The insights of AFs are presented to benefit the researchers for doing further research and practitioners to select among different choices. The code used for experimental comparison is released at: https://github.com/shivram1987/ActivationFunctions.},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/XAFWVEJC/Dubey et al. - 2022 - Activation functions in deep learning A comprehen.pdf}
}

@misc{eigenchrisRelativity108aSchwarzschild,
  title = {(1) {{Relativity}} 108a: {{Schwarzschild Metric}} - {{Derivation}} - {{YouTube}}},
  shorttitle = {(1) {{Relativity}} 108a},
  author = {{eigenchris}},
  abstract = {Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.},
  howpublished = {https://www.youtube.com/},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/CV9PP9BY/watch.html}
}

@book{evansPartialDifferentialEquations2010,
  title = {Partial Differential Equations},
  author = {Evans, Lawrence C.},
  year = {2010},
  series = {Graduate Studies in Mathematics},
  edition = {2nd ed},
  number = {v. 19},
  publisher = {{American Mathematical Society}},
  address = {{Providence, R.I}},
  abstract = {"This is the second edition of the now definitive text on partial differential equations (PDE). It offers a comprehensive survey of modern techniques in the theoretical study of PDE with particular emphasis on nonlinear equations. Its wide scope and clear exposition make it a great text for a graduate course in PDE. For this edition, the author has made numerous changes, including: a new chapter on nonlinear wave equations, more than 80 new exercises, several new sections, and a significantly expanded bibliography."--Publisher's description},
  isbn = {978-0-8218-4974-3},
  lccn = {QA377 .E95 2010},
  keywords = {Differential equations; Partial},
  annotation = {OCLC: ocn465190110}
}

@article{gannonDeepLearningReal2021,
  title = {Deep {{Learning}} with {{Real Data}} and the {{Chaotic Double Pendulum}}},
  author = {Gannon, Dennis},
  year = {2021},
  publisher = {{Indiana University Bloomington}},
  doi = {10.13140/RG.2.2.16670.05443},
  abstract = {In this note we provide a simple illustration of how the `classic' deep learning tool LSTM can be used to model the solution to time dependent differential equations. We begin with an amazingly simple case: computing the trajectory of a projectile launched into the air, encountering wind resistance as it flies and returns to earth. Given a single sample trajectory, the network can demonstrate very realistic behavior for a variety of initial conditions. The second case is the double pendulum which is well known to generate chaotic behavior for certain initial conditions. We show that a LSTM can effectively model the pendulum in the space of initial conditions up until it encounters the chaotic regions.},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/VEM927TL/Gannon - 2021 - Deep Learning with Real Data and the Chaotic Doubl.pdf}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/5TXRERE3/Glorot and Bengio - 2010 - Understanding the difficulty of training deep feed.pdf}
}

@techreport{grohsDeepNeuralNetwork2019,
  title = {Deep Neural Network Approximations for {{Monte Carlo}} Algorithms},
  author = {Grohs, Philipp and Jentzen, Arnulf and Salimova, Diyora},
  year = {2019},
  month = aug,
  eprint = {1908.10828},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  doi = {10.1007/s42985-021-00100-z},
  abstract = {Recently, it has been proposed in the literature to employ deep neural networks (DNNs) together with stochastic gradient descent methods to approximate solutions of PDEs. There are also a few results in the literature which prove that DNNs can approximate solutions of certain PDEs without the curse of dimensionality in the sense that the number of real parameters used to describe the DNN grows at most polynomially both in the PDE dimension and the reciprocal of the prescribed approximation accuracy. One key argument in most of these results is, first, to use a Monte Carlo approximation scheme which can approximate the solution of the PDE under consideration at a fixed space-time point without the curse of dimensionality and, thereafter, to prove that DNNs are flexible enough to mimic the behaviour of the used approximation scheme. Having this in mind, one could aim for a general abstract result which shows under suitable assumptions that if a certain function can be approximated by any kind of (Monte Carlo) approximation scheme without the curse of dimensionality, then this function can also be approximated with DNNs without the curse of dimensionality. It is a key contribution of this article to make a first step towards this direction. In particular, the main result of this paper, essentially, shows that if a function can be approximated by means of some suitable discrete approximation scheme without the curse of dimensionality and if there exist DNNs which satisfy certain regularity properties and which approximate this discrete approximation scheme without the curse of dimensionality, then the function itself can also be approximated with DNNs without the curse of dimensionality. As an application of this result we establish that solutions of suitable Kolmogorov PDEs can be approximated with DNNs without the curse of dimensionality.},
  archiveprefix = {arXiv},
  keywords = {65L70; 68T99; 65C05; 60H30,Computer Science - Machine Learning,Mathematics - Analysis of PDEs,Mathematics - Numerical Analysis,Mathematics - Probability},
  file = {/Users/miles_cb/Zotero/storage/SUN7G4ZX/Grohs et al. - 2019 - Deep neural network approximations for Monte Carlo.pdf;/Users/miles_cb/Zotero/storage/T22IME4X/1908.html}
}

@article{hanSolvingHighdimensionalPartial2018,
  title = {Solving High-Dimensional Partial Differential Equations Using Deep Learning},
  author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
  year = {2018},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {34},
  pages = {8505--8510},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1718942115},
  file = {/Users/miles_cb/Zotero/storage/SQ58KF5A/Han et al. - 2018 - Solving high-dimensional partial differential equa.pdf}
}

@article{hornikApproximationCapabilitiesMultilayer1991,
  title = {Approximation Capabilities of Multilayer Feedforward Networks},
  author = {Hornik, Kurt},
  year = {1991},
  month = jan,
  journal = {Neural Networks},
  volume = {4},
  number = {2},
  pages = {251--257},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(91)90009-T},
  abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp({$\mu$}) performance criteria, for arbitrary finite input environment measures {$\mu$}, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
  langid = {english},
  keywords = {() approximation,Activation function,Input environment measure,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
  file = {/Users/miles_cb/Zotero/storage/EYAJV6VR/089360809190009T.html}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  langid = {english},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/Users/miles_cb/Zotero/storage/IAH6RPEQ/0893608089900208.html}
}

@article{karniadakisPhysicsinformedMachineLearning2021,
  title = {Physics-Informed Machine Learning},
  author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  year = {2021},
  month = jun,
  journal = {Nature Reviews Physics},
  volume = {3},
  number = {6},
  pages = {422--440},
  issn = {2522-5820},
  doi = {10.1038/s42254-021-00314-5},
  abstract = {Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-d imensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-t ime domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-b ased regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-d imensional problems.},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/II85ZM6U/Karniadakis et al. - 2021 - Physics-informed machine learning.pdf}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/miles_cb/Zotero/storage/ZBGZ7U8U/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/miles_cb/Zotero/storage/58RYURSY/1412.html}
}

@techreport{lagarisArtificialNeuralNetworks1997,
  title = {Artificial {{Neural Networks}} for {{Solving Ordinary}} and {{Partial Differential Equations}}},
  author = {Lagaris, I. E. and Likas, A. and Fotiadis, D. I.},
  year = {1997},
  month = may,
  eprint = {physics/9705023},
  eprinttype = {arxiv},
  doi = {10.1109/72.712178},
  abstract = {We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the boundary (or initial) conditions and contains no adjustable parameters. The second part is constructed so as not to affect the boundary conditions. This part involves a feedforward neural network, containing adjustable parameters (the weights). Hence by construction the boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ODE's, to systems of coupled ODE's and also to PDE's. In this article we illustrate the method by solving a variety of model problems and present comparisons with finite elements for several cases of partial differential equations.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Cellular Automata and Lattice Gases,Physics - Computational Physics,Quantum Physics},
  file = {/Users/miles_cb/Zotero/storage/TDDY2ZJL/Lagaris et al. - 1997 - Artificial Neural Networks for Solving Ordinary an.pdf;/Users/miles_cb/Zotero/storage/UX42ZWSQ/9705023.html}
}

@article{laurieCalculationGaussKronrodQuadrature1997,
  title = {Calculation of {{Gauss-Kronrod}} Quadrature Rules},
  author = {Laurie, Dirk},
  year = {1997},
  journal = {Mathematics of Computation},
  volume = {66},
  number = {219},
  pages = {1133--1145},
  issn = {0025-5718, 1088-6842},
  doi = {10.1090/S0025-5718-97-00861-2},
  abstract = {The Jacobi matrix of the (2n+1)-point Gauss-Kronrod quadrature rule for a given measure is calculated efficiently by a five-term recurrence relation. The algorithm uses only rational operations and is therefore also useful for obtaining the Jacobi-Kronrod matrix analytically. The nodes and weights can then be computed directly by standard software for Gaussian quadrature formulas.},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/QDEN338T/Laurie - 1997 - Calculation of Gauss-Kronrod quadrature rules.pdf}
}

@article{liuLimitedMemoryBFGS1989,
  title = {On the Limited Memory {{BFGS}} Method for Large Scale Optimization},
  author = {Liu, Dong C. and Nocedal, Jorge},
  year = {1989},
  month = aug,
  journal = {Mathematical Programming},
  volume = {45},
  number = {1},
  pages = {503--528},
  issn = {1436-4646},
  doi = {10.1007/BF01589116},
  abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
  langid = {english},
  keywords = {conjugate gradient method,Large scale nonlinear optimization,limited memory methods,partitioned quasi-Newton method}
}

@inproceedings{luUniversalApproximationTheorem2020,
  title = {A {{Universal Approximation Theorem}} of {{Deep Neural Networks}} for {{Expressing Probability Distributions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lu, Yulong and Lu, Jianfeng},
  year = {2020},
  volume = {33},
  pages = {3094--3105},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/miles_cb/Zotero/storage/7WX4LDHR/Lu and Lu - 2020 - A Universal Approximation Theorem of Deep Neural N.pdf}
}

@misc{pal2022lux,
  title = {Lux: {{Explicit}} Parameterization of Deep Neural Networks in Julia},
  author = {Pal, Avik},
  year = {2022},
  publisher = {{GitHub}}
}

@article{rackauckasDifferentialEquationsJlPerformant2017,
  title = {{{DifferentialEquations}}.Jl \textendash{} {{A Performant}} and {{Feature-Rich Ecosystem}} for {{Solving Differential Equations}} in {{Julia}}},
  author = {Rackauckas, Christopher and Nie, Qing},
  year = {2017},
  month = may,
  journal = {Journal of Open Research Software},
  volume = {5},
  number = {1},
  pages = {15},
  issn = {2049-9647},
  doi = {10.5334/jors.151},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/9U6KIXE3/Rackauckas and Nie - 2017 - DifferentialEquations.jl – A Performant and Featur.pdf}
}

@misc{rackauckasSciMLSciMLBookParallel,
  title = {{{SciML}}/{{SciMLBook}}: {{Parallel Computing}} and {{Scientific Machine Learning}} ({{SciML}}): {{Methods}} and {{Applications}} ({{MIT}} 18.{{337J}}/6.{{338J}})},
  shorttitle = {{{SciML}}/{{SciMLBook}}},
  author = {Rackauckas, Chris},
  journal = {GitHub},
  abstract = {Parallel Computing and Scientific Machine Learning (SciML): Methods and Applications (MIT 18.337J/6.338J) - SciML/SciMLBook: Parallel Computing and Scientific Machine Learning (SciML): Methods and ...},
  howpublished = {https://github.com/SciML/SciMLBook},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/U585MEGM/SciMLBook.html}
}

@article{raissiPhysicsinformedNeuralNetworks2019,
  title = {Physics-Informed Neural Networks: {{A}} Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  shorttitle = {Physics-Informed Neural Networks},
  author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
  year = {2019},
  month = feb,
  journal = {Journal of Computational Physics},
  volume = {378},
  pages = {686--707},
  issn = {00219991},
  doi = {10.1016/j.jcp.2018.10.045},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/7M2L3634/Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf}
}

@article{riveraQuadratureRulesSolving2022,
  title = {On Quadrature Rules for Solving {{Partial Differential Equations}} Using {{Neural Networks}}},
  author = {Rivera, Jon A. and Taylor, Jamie M. and Omella, {\'A}ngel J. and Pardo, David},
  year = {2022},
  month = apr,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {393},
  pages = {114710},
  issn = {0045-7825},
  doi = {10.1016/j.cma.2022.114710},
  abstract = {Neural Networks have been widely used to solve Partial Differential Equations. These methods require to approximate definite integrals using quadrature rules. Here, we illustrate via 1D numerical examples the quadrature problems that may arise in these applications and propose several alternatives to overcome them, namely: Monte Carlo methods, adaptive integration, polynomial approximations of the Neural Network output, and the inclusion of regularization terms in the loss. We also discuss the advantages and limitations of each proposed numerical integration scheme. We advocate the use of Monte Carlo methods for high dimensions (above 3 or 4), and adaptive integration or polynomial approximations for low dimensions (3 or below). The use of regularization terms is a mathematically elegant alternative that is valid for any spatial dimension; however, it requires certain regularity assumptions on the solution and complex mathematical analysis when dealing with sophisticated Neural Networks.},
  langid = {english},
  keywords = {Deep learning,Least-Squares method,Neural Networks,Quadrature rules,Ritz method},
  file = {/Users/miles_cb/Zotero/storage/KZUBCWW5/Rivera et al. - 2022 - On quadrature rules for solving Partial Differenti.pdf;/Users/miles_cb/Zotero/storage/ZULZ8KLW/S0045782522000810.html}
}

@article{samibatainehApproximateAnalyticalSolutions2008,
  title = {Approximate Analytical Solutions of Systems of {{PDEs}} by Homotopy Analysis Method},
  author = {Sami Bataineh, A. and Noorani, M. S. M. and Hashim, I.},
  year = {2008},
  month = jun,
  journal = {Computers \& Mathematics with Applications},
  volume = {55},
  number = {12},
  pages = {2913--2923},
  issn = {0898-1221},
  doi = {10.1016/j.camwa.2007.11.022},
  abstract = {In this paper, the homotopy analysis method (HAM) is applied to obtain series solutions to linear and nonlinear systems of first- and second-order partial differential equations (PDEs). The HAM solutions contain an auxiliary parameter which provides a convenient way of controlling the convergence region of series solutions. It is shown in particular that the solutions obtained by the variational iteration method (VIM) are only special cases of the HAM solutions.},
  langid = {english},
  keywords = {Adomian decomposition method,Homotopy analysis method,Nonlinear systems,Systems of PDEs,Variational iteration method},
  file = {/Users/miles_cb/Zotero/storage/MZ4YGV53/Sami Bataineh et al. - 2008 - Approximate analytical solutions of systems of PDE.pdf;/Users/miles_cb/Zotero/storage/ZSVLED2I/S0898122107008139.html}
}

@misc{schwarzschildGravitationalFieldMass1999,
  title = {On the Gravitational Field of a Mass Point According to {{Einstein}}'s Theory},
  author = {Schwarzschild, K.},
  year = {1999},
  month = may,
  number = {arXiv:physics/9905030},
  eprint = {physics/9905030},
  eprinttype = {arxiv},
  institution = {{arXiv}},
  abstract = {Translation by S. Antoci and A. Loinger of the fundamental memoir, that contains the ORIGINAL form of the solution of Schwarzschild's problem. The solution is regular in the whole space-time, with the only exception of the origin of the spatial co-ordinates; consequently, it leaves no room for the science fiction of the black holes. (In the centuries of the decline of the Roman Empire people said: ``Graecum est, non legitur''...).},
  archiveprefix = {arXiv},
  keywords = {General Relativity and Quantum Cosmology,Physics - History and Philosophy of Physics},
  file = {/Users/miles_cb/Zotero/storage/9HWE8IKZ/Schwarzschild - 1999 - On the gravitational field of a mass point accordi.pdf;/Users/miles_cb/Zotero/storage/GGPXQA9D/9905030.html}
}

@misc{SciMLCon2022,
  title = {{{SciMLCon}} 2022},
  journal = {SciMLCon 2022},
  abstract = {SciMLCon 2022, Everywhere on Earth},
  howpublished = {https://scimlcon.org/2022/},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/J8FYX8VW/2022.html}
}

@book{strangLinearAlgebraLearning2019,
  title = {Linear Algebra and Learning from Data},
  author = {Strang, Gilbert},
  year = {2019},
  publisher = {{Wellesley-Cambridge press}},
  address = {{Wellesley}},
  isbn = {978-0-692-19638-0},
  langid = {english},
  lccn = {512.5}
}

@misc{voigtlaenderUniversalApproximationTheorem2020,
  title = {The Universal Approximation Theorem for Complex-Valued Neural Networks},
  author = {Voigtlaender, Felix},
  year = {2020},
  month = dec,
  number = {arXiv:2012.03351},
  eprint = {2012.03351},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.03351},
  abstract = {We generalize the classical universal approximation theorem for neural networks to the case of complex-valued neural networks. Precisely, we consider feedforward networks with a complex activation function \$\textbackslash sigma : \textbackslash mathbb\{C\} \textbackslash to \textbackslash mathbb\{C\}\$ in which each neuron performs the operation \$\textbackslash mathbb\{C\}\^N \textbackslash to \textbackslash mathbb\{C\}, z \textbackslash mapsto \textbackslash sigma(b + w\^T z)\$ with weights \$w \textbackslash in \textbackslash mathbb\{C\}\^N\$ and a bias \$b \textbackslash in \textbackslash mathbb\{C\}\$, and with \$\textbackslash sigma\$ applied componentwise. We completely characterize those activation functions \$\textbackslash sigma\$ for which the associated complex networks have the universal approximation property, meaning that they can uniformly approximate any continuous function on any compact subset of \$\textbackslash mathbb\{C\}\^d\$ arbitrarily well. Unlike the classical case of real networks, the set of "good activation functions" which give rise to networks with the universal approximation property differs significantly depending on whether one considers deep networks or shallow networks: For deep networks with at least two hidden layers, the universal approximation property holds as long as \$\textbackslash sigma\$ is neither a polynomial, a holomorphic function, or an antiholomorphic function. Shallow networks, on the other hand, are universal if and only if the real part or the imaginary part of \$\textbackslash sigma\$ is not a polyharmonic function.},
  archiveprefix = {arXiv},
  keywords = {68T07; 41A30; 41A63; 31A30; 30E10,Computer Science - Machine Learning,Mathematics - Functional Analysis,Statistics - Machine Learning},
  file = {/Users/miles_cb/Zotero/storage/YYWGSS9X/Voigtlaender - 2020 - The universal approximation theorem for complex-va.pdf;/Users/miles_cb/Zotero/storage/TJIYEMH2/2012.html}
}

@article{waldvogelFastConstructionFejer2006,
  title = {Fast {{Construction}} of the {{Fej\'er}} and {{Clenshaw}}\textendash{{Curtis Quadrature Rules}}},
  author = {Waldvogel, J{\"o}rg},
  year = {2006},
  month = mar,
  journal = {BIT Numerical Mathematics},
  volume = {46},
  number = {1},
  pages = {195--202},
  issn = {0006-3835, 1572-9125},
  doi = {10.1007/s10543-006-0045-4},
  abstract = {We present an elegant algorithm for stably and quickly generating the weights of Fej\textasciiacute er's quadrature rules and of the Clenshaw-Curtis rule. The weights for an arbitrary number of nodes are obtained as the discrete Fourier transform of an explicitly defined vector of rational or algebraic numbers. Since these rules have the capability of forming nested families, some of them have gained renewed interest in connection with quadrature over multi-dimensional regions.},
  langid = {english},
  file = {/Users/miles_cb/Zotero/storage/9CDQHCNI/Waldvogel - 2006 - Fast Construction of the Fejér and Clenshaw–Curtis.pdf}
}

@misc{wangHiddenConvexOptimization2022,
  title = {The {{Hidden Convex Optimization Landscape}} of {{Two-Layer ReLU Neural Networks}}: An {{Exact Characterization}} of the {{Optimal Solutions}}},
  shorttitle = {The {{Hidden Convex Optimization Landscape}} of {{Two-Layer ReLU Neural Networks}}},
  author = {Wang, Yifei and Lacotte, Jonathan and Pilanci, Mert},
  year = {2022},
  month = mar,
  number = {arXiv:2006.05900},
  eprint = {2006.05900},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2006.05900},
  abstract = {We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all optimal solutions, and does not leverage duality-based analysis which was recently used to lift neural network training into convex spaces. Given the set of solutions of our convex optimization program, we show how to construct exactly the entire set of optimal neural networks. We provide a detailed characterization of this optimal set and its invariant transformations. As additional consequences of our convex perspective, (i) we establish that Clarke stationary points found by stochastic gradient descent correspond to the global optimum of a subsampled convex problem (ii) we provide a polynomial-time algorithm for checking if a neural network is a global minimum of the training loss (iii) we provide an explicit construction of a continuous path between any neural network and the global minimum of its sublevel set and (iv) characterize the minimal size of the hidden layer so that the neural network optimization landscape has no spurious valleys. Overall, we provide a rich framework for studying the landscape of neural network training loss through convexity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/miles_cb/Zotero/storage/M79L3F9C/Wang et al. - 2022 - The Hidden Convex Optimization Landscape of Two-La.pdf;/Users/miles_cb/Zotero/storage/IIZ8ZRML/2006.html}
}

@article{wazwazVariationalIterationMethod2007a,
  title = {The Variational Iteration Method for Solving Linear and Nonlinear Systems of {{PDEs}}},
  author = {Wazwaz, Abdul-Majid},
  year = {2007},
  month = oct,
  journal = {Computers \& Mathematics with Applications},
  series = {Variational {{Iteration Method}} for {{Nonlinear Problems}}},
  volume = {54},
  number = {7},
  pages = {895--902},
  issn = {0898-1221},
  doi = {10.1016/j.camwa.2006.12.059},
  abstract = {In this work, the variational iteration method (VIM) is used for analytic treatment of the linear and nonlinear systems of partial differential equations. The method reduces the calculation size and overcomes the difficulty of handling nonlinear terms. Numerical examples are examined to highlight the significant features of the VIM method. The method shows improvements over existing numerical techniques.},
  langid = {english},
  keywords = {Systems of PDEs,Variational iteration method},
  file = {/Users/miles_cb/Zotero/storage/VK7H3J22/Wazwaz - 2007 - The variational iteration method for solving linea.pdf;/Users/miles_cb/Zotero/storage/3J45UFHD/S0898122107003033.html}
}

@misc{zubovNeuralPDEAutomatingPhysicsInformed2021,
  title = {{{NeuralPDE}}: {{Automating Physics-Informed Neural Networks}} ({{PINNs}}) with {{Error Approximations}}},
  shorttitle = {{{NeuralPDE}}},
  author = {Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luj{\'a}n, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and Vinchhi, Nand and Balakrishnan, Kaushik and Upadhyay, Devesh and Rackauckas, Chris},
  year = {2021},
  month = jul,
  number = {arXiv:2107.09443},
  eprint = {2107.09443},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2107.09443},
  abstract = {Physics-informed neural networks (PINNs) are an increasingly powerful way to solve partial differential equations, generate digital twins, and create neural surrogates of physical models. In this manuscript we detail the inner workings of NeuralPDE.jl and show how a formulation structured around numerical quadrature gives rise to new loss functions which allow for adaptivity towards bounded error tolerances. We describe the various ways one can use the tool, detailing mathematical techniques like using extended loss functions for parameter estimation and operator discovery, to help potential users adopt these PINN-based techniques into their workflow. We showcase how NeuralPDE uses a purely symbolic formulation so that all of the underlying training code is generated from an abstract formulation, and show how to make use of GPUs and solve systems of PDEs. Afterwards we give a detailed performance analysis which showcases the trade-off between training techniques on a large set of PDEs. We end by focusing on a complex multiphysics example, the Doyle-Fuller-Newman (DFN) Model, and showcase how this PDE can be formulated and solved with NeuralPDE. Together this manuscript is meant to be a detailed and approachable technical report to help potential users of the technique quickly get a sense of the real-world performance trade-offs and use cases of the PINN techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Mathematical Software,Computer Science - Symbolic Computation},
  file = {/Users/miles_cb/Zotero/storage/33PXEPM7/Zubov et al. - 2021 - NeuralPDE Automating Physics-Informed Neural Netw.pdf;/Users/miles_cb/Zotero/storage/8IFMQSIW/2107.html}
}

